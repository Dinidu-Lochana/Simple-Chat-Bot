# -*- coding: utf-8 -*-
"""ChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oHEm_NObva3qftyd7KIYdH3hzIPro-x8

# **Chat Bot**
"""

!pip install torch -q
!pip install transformers -q
!pip install numpy -q
!pip install langchain -q
!pip install langchain_community -q
!pip install langchain-chroma -q
!pip install sentence_transformers -q

!pip uninstall -y protobuf tokenizers

!pip install protobuf==3.20.3 tokenizers==0.20.3

!pip install torch transformers numpy langchain langchain_community langchain-chroma sentence_transformers

pip install --upgrade torch transformers numpy langchain langchain_community langchain-chroma sentence_transformers

!pip show protobuf tokenizers

pip install -U langchain-huggingface

import os
from google.colab import userdata

"""# **Initialize HuggingFace LLM**"""

os.environ["HUGGINGFACEHUB_API_TOKEN"] = userdata.get('HUGGINGFACEHUB_API_TOKEN')

from langchain_community.llms import HuggingFaceHub

llm = HuggingFaceHub(
    repo_id = "mistralai/Mistral-7B-v0.1",
    model_kwargs = {"temperature": 0.1, "max_length": 512},
    huggingfacehub_api_token = os.environ["HUGGINGFACEHUB_API_TOKEN"]
)

"""# **Initialize Embedding Model**"""

from langchain.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(
    model_name = "sentence-transformers/all-mpnet-base-v2"
)

"""# **Initialize Output Parser**"""

from langchain_core.output_parsers import StrOutputParser

# Output parser
output_parser = StrOutputParser()

"""# **Load PDF Document**"""

!pip install pypdf -qU

from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("/content/Deep_Learning_Machine_Learning_Updated.pdf")

docs = loader.load()

len(docs)

"""# **Spliting Documets into Chuncks**"""

from langchain_text_splitters import RecursiveCharacterTextSplitter

# Text Splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 400, chunk_overlap = 50)

# Split
splits = text_splitter.split_documents(docs)

len(splits)

"""# **Vector Store and Retriever**"""

from langchain_chroma import Chroma

vectorStore = Chroma.from_documents(
    documents = splits,
    embedding = embedding_model
)

retriver = vectorStore.as_retriever()

"""# **Prompt Template**"""

from langchain.prompts import ChatPromptTemplate

template= """
Answer this question using the provided context only.

{question}

Context:
{context}

Answer:
"""

promptTemplate = ChatPromptTemplate.from_template(template)

promptTemplate

"""# **Chain Retriver and Prompt Template with LLM**"""

from langchain.schema.runnable import RunnablePassthrough

chain = (
    {"context" : retriver, "question" : RunnablePassthrough()}
    | promptTemplate
    | llm
    | output_parser
)

"""# **Invoke RAG Chain with Inputs**"""

response = chain.invoke("What is deep learning?")

print(response)

response = chain.invoke("What is Machine Learning?")

print(response)

